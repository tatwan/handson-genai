{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e741da05",
   "metadata": {},
   "source": [
    "# Basic Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fb4d463d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original sentence: NLP is a fascinating field of study.\n",
      "Tokens: ['nlp', 'is', 'a', 'fascinating', 'field', 'of', 'study.']\n"
     ]
    }
   ],
   "source": [
    "# Our sample sentence\n",
    "sentence = \"NLP is a fascinating field of study.\"\n",
    "print(f\"Original sentence: {sentence}\")\n",
    "\n",
    "# A very simple tokenizer: convert to lowercase and split by spaces\n",
    "tokens = sentence.lower().split(' ')\n",
    "\n",
    "print(f\"Tokens: {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "943e7ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'you', 'love', 'nlp?', \"it's\", 'a', 'fascinating', 'field!']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Don't you love NLP? It's a fascinating field!\"\n",
    "\n",
    "# Tokenize using the split() method\n",
    "tokens = sentence.lower().split(' ')\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671c14c5",
   "metadata": {},
   "source": [
    "Notice the problems. The punctuation is still attached to the words (`\"nlp?\"`, `\"field!\"`), and contractions like `\"don't\"` and `\"it's\"` are treated as single, unchangeable units. This method is fast but not very smart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e12d027",
   "metadata": {},
   "source": [
    "## Using NLTK\n",
    "\n",
    "The __Natural Language Toolkit (NLTK)__ is a foundational library for NLP education and research. Its word_tokenize function is trained to handle many edge cases, like punctuation and contractions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0f2b7e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c0b123ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tarekatwan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "262c1bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'you', 'love', 'NLP', '?', 'It', \"'s\", 'a', 'fascinating', 'field', '!']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sentence = \"Don't you love NLP? It's a fascinating field!\"\n",
    "\n",
    "# Tokenize using NLTK's word_tokenize\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc11e03",
   "metadata": {},
   "source": [
    "NLTK correctly separates punctuation (`?`, `!`) from the words. It also intelligently splits the contraction `\"Don't\"` into its components `Do` and `n't`, which is crucial for understanding the sentence's components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e021ad74",
   "metadata": {},
   "source": [
    "## spaCy (A Modern, Production-Ready Library)\n",
    "spaCy is a modern, high-performance NLP library designed for real-world applications. Its tokenizer is fast and efficient, and it's part of a larger pipeline that creates rich document objects, where each token has useful linguistic annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "37e5cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.3 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.3 which is incompatible.\n",
      "numba 0.61.2 requires numpy<2.3,>=1.24, but you have numpy 2.3.3 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy -q\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c213ade8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'you', 'love', 'NLP', '?', 'It', \"'s\", 'a', 'fascinating', 'field', '!']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the small English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sentence = \"Don't you love NLP? It's a fascinating field!\"\n",
    "\n",
    "# Process the sentence with the spaCy pipeline\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# The 'doc' object is a sequence of tokens. We can extract their text.\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f23b2fd",
   "metadata": {},
   "source": [
    "# Bag-ofWords from Stratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "66614466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our corpus of documents\n",
    "corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog chased the cat.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "844ebdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Building the Vocabulary ---\n"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Build the Vocabulary ---\n",
    "# We'll collect all unique words from the corpus in a single set\n",
    "print(\"--- Step 1: Building the Vocabulary ---\")\n",
    "all_words = set()\n",
    "for sentence in corpus:\n",
    "    # simple tokenization: lowercase and split by space\n",
    "    tokens = sentence.lower().replace('.', '').split(' ')\n",
    "    for word in tokens:\n",
    "        all_words.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bff4ffca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cat', 'chased', 'dog', 'mat', 'on', 'sat', 'the'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "a5ef326c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Vocabulary: ['cat', 'chased', 'dog', 'mat', 'on', 'sat', 'the']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert the set to a sorted list to have a consistent order\n",
    "vocabulary = sorted(list(all_words))\n",
    "print(f\"Final Vocabulary: {vocabulary}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "09884d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 2: Creating the Vectors ---\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Create the Vectors ---\n",
    "# We'll create a vector for each sentence by counting word occurrences\n",
    "print(\"--- Step 2: Creating the Vectors ---\")\n",
    "final_vectors = []\n",
    "for sentence in corpus:\n",
    "    # Start with a vector of zeros, one position for each word in the vocabulary\n",
    "    vector = [0] * len(vocabulary)\n",
    "    \n",
    "    # Tokenize the current sentence\n",
    "    sentence_tokens = sentence.lower().replace('.', '').split(' ')\n",
    "    \n",
    "    # Count the words\n",
    "    for word in sentence_tokens:\n",
    "        # Find the index of the word in our vocabulary\n",
    "        if word in vocabulary:\n",
    "            index = vocabulary.index(word)\n",
    "            # Increment the count at that index\n",
    "            vector[index] += 1\n",
    "            \n",
    "    final_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19337c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1 Vector: [1, 0, 0, 1, 1, 1, 2]\n",
      "Sentence 2 Vector: [1, 1, 1, 0, 0, 0, 2]\n"
     ]
    }
   ],
   "source": [
    "# Print the results beautifully\n",
    "print(\"Sentence 1 Vector:\", final_vectors[0])\n",
    "print(\"Sentence 2 Vector:\", final_vectors[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c9295af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Readable DataFrame ---\n",
      "            cat  chased  dog  mat  on  sat  the\n",
      "Sentence 1    1       0    0    1   1    1    2\n",
      "Sentence 2    1       1    1    0   0    0    2\n"
     ]
    }
   ],
   "source": [
    "# For comparison with the scikit-learn output\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(final_vectors, columns=vocabulary, index=['Sentence 1', 'Sentence 2'])\n",
    "print(\"\\n--- Readable DataFrame ---\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6a0149",
   "metadata": {},
   "source": [
    "# Bag-of-Words with Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9b77c051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned Vocabulary: ['cat' 'chased' 'dog' 'mat' 'on' 'sat' 'the']\n"
     ]
    }
   ],
   "source": [
    "# Import the CountVectorizer class from scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Our corpus of documents\n",
    "corpus = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog chased the cat.\"\n",
    "]\n",
    "\n",
    "# 1. Create an instance of CountVectorizer\n",
    "# This object will learn the vocabulary and generate vectors\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# 2. Fit the vectorizer to the corpus and transform the corpus into vectors\n",
    "# .fit_transform() learns the vocabulary and returns the document-term matrix (our vectors)\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# 3. Get the learned vocabulary\n",
    "# The vocabulary is a dictionary where keys are words and values are their index positions in the vector\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "print(f\"Learned Vocabulary: {vocabulary}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "05a84800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resulting Vectors (Document-Term Matrix):\n",
      "[[1 0 0 1 1 1 2]\n",
      " [1 1 1 0 0 0 2]]\n"
     ]
    }
   ],
   "source": [
    "# 4. View the vectors\n",
    "# The result is a sparse matrix. We convert it to a dense array for readability.\n",
    "vectors = X.toarray()\n",
    "print(\"\\nResulting Vectors (Document-Term Matrix):\")\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a84f8f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Readable DataFrame ---\n",
      "            cat  chased  dog  mat  on  sat  the\n",
      "Sentence 1    1       0    0    1   1    1    2\n",
      "Sentence 2    1       1    1    0   0    0    2\n"
     ]
    }
   ],
   "source": [
    "# For a more readable output, let's put it in a pandas DataFrame\n",
    "df = pd.DataFrame(vectors, columns=vocabulary, index=['Sentence 1', 'Sentence 2'])\n",
    "print(\"\\n--- Readable DataFrame ---\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77cc7447",
   "metadata": {},
   "source": [
    "#  BoW Limitation (Loss of Word Order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "90b92727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Vocabulary and Vectors ---\n",
      "            bit  dog  man  the\n",
      "Sentence A    1    1    1    2\n",
      "Sentence B    1    1    1    2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Two sentences with completely different meanings\n",
    "sentences_with_different_meanings = [\n",
    "    \"The dog bit the man.\",\n",
    "    \"The man bit the dog.\"\n",
    "]\n",
    "\n",
    "# Use the same CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences_with_different_meanings)\n",
    "\n",
    "# Display the results\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "df = pd.DataFrame(X.toarray(), columns=vocabulary, index=['Sentence A', 'Sentence B'])\n",
    "\n",
    "print(\"--- Vocabulary and Vectors ---\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b51f4",
   "metadata": {},
   "source": [
    "Although the sentences mean opposite things, their Bag-of-Words representations are identical. This proves that the model has lost the crucial information contained in the word order. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce7422",
   "metadata": {},
   "source": [
    "# Word2Vec\n",
    "## Training Your Own Mini Word2Vec Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "883466d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "5b98db4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/tarekatwan/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9ca641fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/tarekatwan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt') # Download tokenizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7b74bdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Sample Corpus\n",
    "corpus = [\n",
    "    \"Generative AI is a powerful technology.\",\n",
    "    \"Large language models can create human-like text.\",\n",
    "    \"The transformer architecture revolutionized NLP.\",\n",
    "    \"Deep learning models require significant data.\",\n",
    "    \"Artificial intelligence is a broad field of study.\",\n",
    "    \"Transformers are the basis for models like GPT.\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "29634524",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['generative', 'ai', 'is', 'a', 'powerful', 'technology', '.'],\n",
       " ['large', 'language', 'models', 'can', 'create', 'human-like', 'text', '.'],\n",
       " ['the', 'transformer', 'architecture', 'revolutionized', 'nlp', '.'],\n",
       " ['deep', 'learning', 'models', 'require', 'significant', 'data', '.'],\n",
       " ['artificial',\n",
       "  'intelligence',\n",
       "  'is',\n",
       "  'a',\n",
       "  'broad',\n",
       "  'field',\n",
       "  'of',\n",
       "  'study',\n",
       "  '.'],\n",
       " ['transformers', 'are', 'the', 'basis', 'for', 'models', 'like', 'gpt', '.']]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Preprocess Data (Tokenization)\n",
    "tokenized_corpus = [word_tokenize(doc.lower()) for doc in corpus]\n",
    "tokenized_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc11d4ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 460)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Train Word2Vec Model\n",
    "# vector_size: dimensionality of the word vectors\n",
    "# window: max distance between the current and predicted word within a sentence\n",
    "# min_count: ignores all words with total frequency lower than this\n",
    "model = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.train(tokenized_corpus, total_examples=len(tokenized_corpus), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0205a0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create Document Vectors\n",
    "# We'll use a simple approach: average the word vectors for each document\n",
    "def get_doc_vector(doc_tokens, model):\n",
    "    word_vectors = [model.wv[word] for word in doc_tokens if word in model.wv]\n",
    "    if not word_vectors:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(word_vectors, axis=0)\n",
    "\n",
    "doc_vectors = [get_doc_vector(doc, model) for doc in tokenized_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "a7aa74f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Document: 'Generative AI is a powerful technology.'\n",
      "Most Similar Document: 'Artificial intelligence is a broad field of study.'\n",
      "Similarity Score: 0.3348\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Calculate and Find Most Similar Document (scikit-learn approach) ---\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Let's find documents similar to the first one\n",
    "query_vector = doc_vectors[0]\n",
    "\n",
    "# scikit-learn's function expects 2D arrays, so we reshape the query vector\n",
    "# The output is a 2D array, so we access the first (and only) row with [0]\n",
    "similarities = cosine_similarity(query_vector.reshape(1, -1), doc_vectors)[0]\n",
    "\n",
    "# Find the most similar document (excluding itself)\n",
    "most_similar_idx = np.argsort(similarities)[-2] # -1 is the document itself\n",
    "\n",
    "print(f\"Original Document: '{corpus[0]}'\")\n",
    "print(f\"Most Similar Document: '{corpus[most_similar_idx]}'\")\n",
    "print(f\"Similarity Score: {similarities[most_similar_idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216ca531",
   "metadata": {},
   "source": [
    "## Pre-trained Word2Vec\n",
    "In this lab, we'll use a pre-trained Word2Vec model to find the most similar document to a given query. Instead of training a model ourselves (which takes a lot of data and time), we'll use a model trained on a massive dataset (like all of Wikipedia).\n",
    "\n",
    "Our strategy will be to represent each document by taking the average of the word vectors of all the words within it. This gives us a single vector that captures the document's overall meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3e92a2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Setup and Load the Pre-trained Model ---\n",
    "# We use gensim's downloader to fetch a pre-trained model.\n",
    "# 'glove-wiki-gigaword-50' is a small model with 50-dimensional vectors.\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1e160a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 2: Define Our Documents ---\n",
    "# We have three documents with different topics.\n",
    "documents = [\n",
    "    \"The sun is the star at the center of the Solar System.\", # About space\n",
    "    \"The ocean is a body of salt water that covers most of the Earth.\", # About oceans\n",
    "    \"A computer is a machine that can be programmed to carry out sequences of arithmetic or logical operations.\" # About technology\n",
    "]\n",
    "doc_labels = [\"Space Document\", \"Ocean Document\", \"Technology Document\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "497ddb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Create a Function to Vectorize a Document ---\n",
    "# This function converts a document into a single vector by averaging its word vectors.\n",
    "def vectorize_document(doc, model):\n",
    "    \"\"\"Converts a document string into a single averaged vector.\"\"\"\n",
    "    words = doc.lower().split()\n",
    "    \n",
    "    # Get the vector for each word in the document, if the word exists in the model\n",
    "    word_vectors = [model.wv[word] for word in words if word in model.wv]\n",
    "    \n",
    "    if not word_vectors:\n",
    "        # If no words in the document are in the model's vocabulary, return a vector of zeros\n",
    "        return np.zeros(model.vector_size)\n",
    "    \n",
    "    # Return the mean of the word vectors to get a single document vector\n",
    "    return np.mean(word_vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c0001055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4: Vectorize All Our Documents ---\n",
    "doc_vectors = [vectorize_document(doc, model) for doc in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f5104003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'The astronaut travels to the moon in a rocket.'\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: Define a Query and Find the Most Similar Document ---\n",
    "query = \"The astronaut travels to the moon in a rocket.\"\n",
    "print(f\"\\nQuery: '{query}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "c8da91e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the query using the same function\n",
    "query_vector = vectorize_document(query, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "4cc84249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between the query vector and all document vectors\n",
    "# We need to reshape the vectors for the function to work correctly.\n",
    "similarities = cosine_similarity(query_vector.reshape(1, -1), doc_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "e0d44f8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Scores: [0.8444814 0.7737638 0.4348228]\n",
      "The most similar document is: 'Space Document'\n",
      "Document Text: 'The sun is the star at the center of the Solar System.'\n"
     ]
    }
   ],
   "source": [
    "# Find the index of the most similar document\n",
    "most_similar_index = np.argmax(similarities)\n",
    "\n",
    "print(f\"\\nSimilarity Scores: {similarities[0]}\")\n",
    "print(f\"The most similar document is: '{doc_labels[most_similar_index]}'\")\n",
    "print(f\"Document Text: '{documents[most_similar_index]}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca0a5ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
